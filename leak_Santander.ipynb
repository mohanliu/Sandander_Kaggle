{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from scipy.stats import mode, skew, kurtosis, entropy\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm.pandas(tqdm_notebook)\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohan/miniconda3/envs/research/lib/python3.6/site-packages/pandas/io/feather_format.py:112: FutureWarning: `nthreads` argument is deprecated, pass `use_threads` instead\n",
      "  return feather.read_dataframe(path, nthreads=nthreads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data loaded from feather 2.27 s!\n",
      "test data loaded from feather 21.80 s!\n"
     ]
    }
   ],
   "source": [
    "def load_data(conf):\n",
    "    st = time.time()\n",
    "    if os.path.exists(f'{conf}.feather'):\n",
    "        out = pd.read_feather(f'{conf}.feather')\n",
    "        print(f'{conf} data loaded from feather {time.time()-st:.2f} s!')\n",
    "    else:\n",
    "        out = pd.read_csv(f'./{conf}.csv.zip')\n",
    "        print(f'{conf} data loaded from csv {time.time()-st:.2f} s!')\n",
    "        out.to_feather(f'{conf}.feather')\n",
    "        print(f'{conf} data save to feather {time.time()-st:.2f} s!')  \n",
    "    return out \n",
    "train = load_data('train')\n",
    "test = load_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transact_cols = [f for f in train.columns if f not in [\"ID\", \"target\"]]\n",
    "y = np.log1p(train[\"target\"]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['f190486d6', '58e2e02e6', 'eeb9cd3aa', '9fd594eec', '6eef030c1',\n",
    "       '15ace8c9f', 'fb0f5dbfe', '58e056e12', '20aa07010', '024c577b9',\n",
    "       'd6bb78916', 'b43a7cfd5', '58232a6fb', '1702b5bf0', '324921c7b', \n",
    "       '62e59a501', '2ec5b290f', '241f0f867', 'fb49e4212',  '66ace2992',\n",
    "       'f74e8f13d', '5c6487af1', '963a49cdc', '26fc93eb7', '1931ccfdd', \n",
    "       '703885424', '70feb1494', '491b9ee45', '23310aa6f', 'e176a204a',\n",
    "       '6619d81fc', '1db387535', 'fc99f9426', '91f701ba2',  '0572565c2',\n",
    "       '190db8488',  'adb64ff71', 'c47340d97', 'c5a231d81', '0ff32eb98'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.04 s, sys: 8.94 s, total: 13 s\n",
      "Wall time: 14.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test[\"target\"] = train[\"target\"].mean()\n",
    "\n",
    "all_df = pd.concat([train[[\"ID\", \"target\"] + cols], test[[\"ID\", \"target\"]+ cols]]).reset_index(drop=True)\n",
    "all_df.head()\n",
    "all_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "CPU_CORES = 1\n",
    "def _time_series_info(row):\n",
    "    try:\n",
    "        id_1st_nz = row.nonzero()[0][0]\n",
    "        value_1st_nz = str(row[id_1st_nz])\n",
    "    except:\n",
    "        return '0_0.0'   \n",
    "    return str(id_1st_nz)+'_'+value_1st_nz\n",
    "\n",
    "def _join2str(df):\n",
    "    return df.apply(lambda x: \"_\".join(x.round(2).astype(str)), axis=1)\n",
    "\n",
    "def _get_leak(df, cols, lag=0, n_thread=2):\n",
    "    \"\"\"\n",
    "    Get leaked data\n",
    "    \"\"\"\n",
    "    st = time.time()\n",
    "    \n",
    "    df_split = np.array_split(df[cols[lag+2:]], n_thread)\n",
    "    df_shift_split = np.array_split(df[cols].shift(lag+2, axis=1)[cols[lag+2:]], n_thread)\n",
    "    \n",
    "    print(f'Shift columns: {time.time()-st:.2f} seconds!')\n",
    "    with Pool(processes=n_thread) as p:\n",
    "        result1 = p.map(_join2str, df_split)\n",
    "        result2 = p.map(_join2str, df_shift_split)\n",
    "        \n",
    "    series_str = pd.concat(list(result1), ignore_index=True)\n",
    "    series_shifted_str = pd.concat(list(result2), ignore_index=True)\n",
    "    print(f'Create time series strings before and after shift: {time.time()-st:.2f} seconds!')\n",
    "    \n",
    "    st = time.time()\n",
    "    series_dict = {}\n",
    "    for i in range(len(series_str)):\n",
    "        key = series_str[i]\n",
    "        if key in series_dict.keys():\n",
    "            continue\n",
    "        series_dict[key] = i\n",
    "    print(f'Create dictionary for faster search: {time.time()-st:.2f} seconds!')\n",
    "    \n",
    "    st = time.time()\n",
    "    target_vals = series_shifted_str.apply(lambda x: df.loc[series_dict[x], cols[lag]] \n",
    "                                                   if x in series_dict else 0)\n",
    "    print(f'Matching process finished: {time.time()-st:.2f} seconds!')\n",
    "    return target_vals\n",
    "\n",
    "def get_all_leak(df, cols=None, nlags=15):\n",
    "    \"\"\"\n",
    "    We just recursively fetch target value for different lags\n",
    "    \"\"\"\n",
    "    df =  df.copy()\n",
    "    \n",
    "    for i in range(nlags):\n",
    "        print(\"Processing lag {}\".format(i))\n",
    "        df[\"leaked_target_\"+str(i)] = _get_leak(df, cols, i)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shift columns: 0.11 seconds!\n",
      "Create time series strings before and after shift: 18.39 seconds!\n",
      "Create dictionary for faster search: 0.75 seconds!\n",
      "Matching process finished: 0.67 seconds!\n",
      "CPU times: user 1.73 s, sys: 342 ms, total: 2.07 s\n",
      "Wall time: 19.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## test the speed of get leaked data of one lag value\n",
    "d = _get_leak(all_df, cols, 0)\n",
    "test_ = all_df\n",
    "test_['predict'] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lag 0\n",
      "Shift columns: 0.12 seconds!\n",
      "Create time series strings before and after shift: 23.21 seconds!\n",
      "Create dictionary for faster search: 0.93 seconds!\n",
      "Matching process finished: 0.50 seconds!\n",
      "Processing lag 1\n",
      "Shift columns: 0.14 seconds!\n",
      "Create time series strings before and after shift: 15.73 seconds!\n",
      "Create dictionary for faster search: 1.00 seconds!\n",
      "Matching process finished: 0.48 seconds!\n",
      "Processing lag 2\n",
      "Shift columns: 0.14 seconds!\n",
      "Create time series strings before and after shift: 15.55 seconds!\n",
      "Create dictionary for faster search: 0.93 seconds!\n",
      "Matching process finished: 0.48 seconds!\n",
      "Processing lag 3\n",
      "Shift columns: 0.14 seconds!\n",
      "Create time series strings before and after shift: 18.76 seconds!\n",
      "Create dictionary for faster search: 0.95 seconds!\n",
      "Matching process finished: 0.51 seconds!\n",
      "Processing lag 4\n",
      "Shift columns: 0.15 seconds!\n",
      "Create time series strings before and after shift: 22.86 seconds!\n",
      "Create dictionary for faster search: 1.27 seconds!\n",
      "Matching process finished: 0.73 seconds!\n",
      "Processing lag 5\n",
      "Shift columns: 0.21 seconds!\n",
      "Create time series strings before and after shift: 18.65 seconds!\n",
      "Create dictionary for faster search: 0.97 seconds!\n",
      "Matching process finished: 0.48 seconds!\n",
      "Processing lag 6\n",
      "Shift columns: 0.14 seconds!\n",
      "Create time series strings before and after shift: 15.54 seconds!\n",
      "Create dictionary for faster search: 0.91 seconds!\n",
      "Matching process finished: 0.48 seconds!\n",
      "Processing lag 7\n",
      "Shift columns: 0.14 seconds!\n",
      "Create time series strings before and after shift: 15.77 seconds!\n",
      "Create dictionary for faster search: 1.38 seconds!\n",
      "Matching process finished: 0.75 seconds!\n",
      "Processing lag 8\n",
      "Shift columns: 0.21 seconds!\n",
      "Create time series strings before and after shift: 25.03 seconds!\n",
      "Create dictionary for faster search: 1.27 seconds!\n",
      "Matching process finished: 0.56 seconds!\n",
      "Processing lag 9\n",
      "Shift columns: 0.16 seconds!\n",
      "Create time series strings before and after shift: 22.41 seconds!\n",
      "Create dictionary for faster search: 1.03 seconds!\n",
      "Matching process finished: 0.49 seconds!\n",
      "Processing lag 10\n",
      "Shift columns: 0.13 seconds!\n",
      "Create time series strings before and after shift: 22.04 seconds!\n",
      "Create dictionary for faster search: 1.55 seconds!\n",
      "Matching process finished: 0.78 seconds!\n",
      "Processing lag 11\n",
      "Shift columns: 0.14 seconds!\n",
      "Create time series strings before and after shift: 21.05 seconds!\n",
      "Create dictionary for faster search: 1.11 seconds!\n",
      "Matching process finished: 0.71 seconds!\n",
      "Processing lag 12\n",
      "Shift columns: 0.22 seconds!\n",
      "Create time series strings before and after shift: 19.21 seconds!\n",
      "Create dictionary for faster search: 1.01 seconds!\n",
      "Matching process finished: 0.53 seconds!\n",
      "Processing lag 13\n",
      "Shift columns: 0.14 seconds!\n",
      "Create time series strings before and after shift: 18.12 seconds!\n",
      "Create dictionary for faster search: 1.15 seconds!\n",
      "Matching process finished: 0.66 seconds!\n",
      "Processing lag 14\n",
      "Shift columns: 0.15 seconds!\n",
      "Create time series strings before and after shift: 23.81 seconds!\n",
      "Create dictionary for faster search: 1.39 seconds!\n",
      "Matching process finished: 0.62 seconds!\n",
      "Processing lag 15\n",
      "Shift columns: 0.13 seconds!\n",
      "Create time series strings before and after shift: 18.42 seconds!\n",
      "Create dictionary for faster search: 0.93 seconds!\n",
      "Matching process finished: 0.50 seconds!\n",
      "Processing lag 16\n",
      "Shift columns: 0.16 seconds!\n",
      "Create time series strings before and after shift: 14.87 seconds!\n",
      "Create dictionary for faster search: 0.93 seconds!\n",
      "Matching process finished: 0.50 seconds!\n",
      "Processing lag 17\n",
      "Shift columns: 0.13 seconds!\n",
      "Create time series strings before and after shift: 14.60 seconds!\n",
      "Create dictionary for faster search: 0.90 seconds!\n",
      "Matching process finished: 0.50 seconds!\n",
      "Processing lag 18\n",
      "Shift columns: 0.13 seconds!\n",
      "Create time series strings before and after shift: 15.23 seconds!\n",
      "Create dictionary for faster search: 1.22 seconds!\n",
      "Matching process finished: 0.67 seconds!\n",
      "Processing lag 19\n",
      "Shift columns: 0.15 seconds!\n",
      "Create time series strings before and after shift: 16.67 seconds!\n",
      "Create dictionary for faster search: 1.27 seconds!\n",
      "Matching process finished: 0.66 seconds!\n",
      "Processing lag 20\n",
      "Shift columns: 0.13 seconds!\n",
      "Create time series strings before and after shift: 16.32 seconds!\n",
      "Create dictionary for faster search: 1.25 seconds!\n",
      "Matching process finished: 0.70 seconds!\n",
      "Processing lag 21\n",
      "Shift columns: 0.14 seconds!\n",
      "Create time series strings before and after shift: 16.96 seconds!\n",
      "Create dictionary for faster search: 1.03 seconds!\n",
      "Matching process finished: 0.53 seconds!\n",
      "Processing lag 22\n",
      "Shift columns: 0.16 seconds!\n",
      "Create time series strings before and after shift: 17.36 seconds!\n",
      "Create dictionary for faster search: 1.21 seconds!\n",
      "Matching process finished: 0.70 seconds!\n",
      "Processing lag 23\n",
      "Shift columns: 0.16 seconds!\n",
      "Create time series strings before and after shift: 17.94 seconds!\n",
      "Create dictionary for faster search: 1.21 seconds!\n",
      "Matching process finished: 0.70 seconds!\n",
      "Processing lag 24\n",
      "Shift columns: 0.15 seconds!\n",
      "Create time series strings before and after shift: 19.95 seconds!\n",
      "Create dictionary for faster search: 1.29 seconds!\n",
      "Matching process finished: 0.70 seconds!\n",
      "Processing lag 25\n",
      "Shift columns: 0.16 seconds!\n",
      "Create time series strings before and after shift: 18.35 seconds!\n",
      "Create dictionary for faster search: 1.25 seconds!\n",
      "Matching process finished: 0.78 seconds!\n",
      "Processing lag 26\n",
      "Shift columns: 0.16 seconds!\n",
      "Create time series strings before and after shift: 18.42 seconds!\n",
      "Create dictionary for faster search: 1.18 seconds!\n",
      "Matching process finished: 0.71 seconds!\n",
      "Processing lag 27\n",
      "Shift columns: 0.15 seconds!\n",
      "Create time series strings before and after shift: 18.32 seconds!\n",
      "Create dictionary for faster search: 1.20 seconds!\n",
      "Matching process finished: 0.73 seconds!\n",
      "Processing lag 28\n",
      "Shift columns: 0.15 seconds!\n",
      "Create time series strings before and after shift: 18.74 seconds!\n",
      "Create dictionary for faster search: 1.18 seconds!\n",
      "Matching process finished: 0.73 seconds!\n",
      "Processing lag 29\n",
      "Shift columns: 0.15 seconds!\n",
      "Create time series strings before and after shift: 19.53 seconds!\n",
      "Create dictionary for faster search: 1.18 seconds!\n",
      "Matching process finished: 0.75 seconds!\n",
      "Processing lag 30\n",
      "Shift columns: 0.15 seconds!\n",
      "Create time series strings before and after shift: 18.62 seconds!\n",
      "Create dictionary for faster search: 1.21 seconds!\n",
      "Matching process finished: 0.80 seconds!\n",
      "Processing lag 31\n",
      "Shift columns: 0.15 seconds!\n",
      "Create time series strings before and after shift: 18.42 seconds!\n",
      "Create dictionary for faster search: 1.17 seconds!\n",
      "Matching process finished: 0.80 seconds!\n",
      "Processing lag 32\n",
      "Shift columns: 0.14 seconds!\n",
      "Create time series strings before and after shift: 18.35 seconds!\n",
      "Create dictionary for faster search: 1.23 seconds!\n",
      "Matching process finished: 0.81 seconds!\n",
      "Processing lag 33\n",
      "Shift columns: 0.14 seconds!\n",
      "Create time series strings before and after shift: 18.16 seconds!\n",
      "Create dictionary for faster search: 1.20 seconds!\n",
      "Matching process finished: 0.89 seconds!\n",
      "Processing lag 34\n",
      "Shift columns: 0.14 seconds!\n",
      "Create time series strings before and after shift: 18.10 seconds!\n",
      "Create dictionary for faster search: 1.19 seconds!\n",
      "Matching process finished: 0.88 seconds!\n",
      "Processing lag 35\n",
      "Shift columns: 0.14 seconds!\n",
      "Create time series strings before and after shift: 18.31 seconds!\n",
      "Create dictionary for faster search: 1.18 seconds!\n",
      "Matching process finished: 0.92 seconds!\n",
      "Processing lag 36\n",
      "Shift columns: 0.13 seconds!\n",
      "Create time series strings before and after shift: 17.98 seconds!\n",
      "Create dictionary for faster search: 1.18 seconds!\n",
      "Matching process finished: 1.00 seconds!\n",
      "Processing lag 37\n",
      "Shift columns: 0.13 seconds!\n",
      "Create time series strings before and after shift: 18.31 seconds!\n",
      "Create dictionary for faster search: 1.16 seconds!\n",
      "Matching process finished: 1.04 seconds!\n",
      "CPU times: user 1min 26s, sys: 14.6 s, total: 1min 41s\n",
      "Wall time: 13min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NLAGS = 38 #Increasing this might help push score a bit\n",
    "all_df = get_all_leak(all_df, cols=cols, nlags=NLAGS)\n",
    "\n",
    "leaky_cols = [\"leaked_target_\"+str(i) for i in range(NLAGS)]\n",
    "train = train.join(all_df.set_index(\"ID\")[leaky_cols], on=\"ID\", how=\"left\")\n",
    "test = test.join(all_df.set_index(\"ID\")[leaky_cols], on=\"ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.6 s, sys: 1.31 s, total: 37.9 s\n",
      "Wall time: 38.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## post-processing\n",
    "cnt = 0\n",
    "for i in range(4300):\n",
    "    tar = train['target'].iloc[i]\n",
    "    nz_idx = train[leaky_cols].iloc[i].nonzero()[0]\n",
    "    try:\n",
    "        likely = train[leaky_cols].iloc[i][nz_idx].value_counts().reset_index().sort_values(by=i, ascending=False).iloc[0,0]\n",
    "    except:\n",
    "        likely = 0.0\n",
    "\n",
    "    if tar == likely:\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3179"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 20s, sys: 16 s, total: 1min 36s\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train[\"nonzero_mean\"] = train[transact_cols].apply(lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1)\n",
    "test[\"nonzero_mean\"] = test[transact_cols].apply(lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leak values found in train and test  4383 43394\n",
      "% of correct leaks values in train  0.8188455395847593\n",
      "CPU times: user 8.61 s, sys: 9.2 s, total: 17.8 s\n",
      "Wall time: 15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#We start with 1st lag target and recusrsively fill zero's\n",
    "train[\"compiled_leak\"] = 0\n",
    "test[\"compiled_leak\"] = 0\n",
    "for i in range(NLAGS):\n",
    "    train.loc[train[\"compiled_leak\"] == 0, \"compiled_leak\"] = train.loc[train[\"compiled_leak\"] == 0, \"leaked_target_\"+str(i)]\n",
    "    test.loc[test[\"compiled_leak\"] == 0, \"compiled_leak\"] = test.loc[test[\"compiled_leak\"] == 0, \"leaked_target_\"+str(i)]\n",
    "    \n",
    "print(\"Leak values found in train and test \", sum(train[\"compiled_leak\"] > 0), sum(test[\"compiled_leak\"] > 0))\n",
    "print(\"% of correct leaks values in train \", sum(train[\"compiled_leak\"] == train[\"target\"])/sum(train[\"compiled_leak\"] > 0))\n",
    "\n",
    "train.loc[train[\"compiled_leak\"] == 0, \"compiled_leak\"] = train.loc[train[\"compiled_leak\"] == 0, \"nonzero_mean\"]\n",
    "test.loc[test[\"compiled_leak\"] == 0, \"compiled_leak\"] = test.loc[test[\"compiled_leak\"] == 0, \"nonzero_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8970406264797846"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "np.sqrt(mean_squared_error(y, np.log1p(train[\"compiled_leak\"]).fillna(14.49)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohan/miniconda3/envs/research/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "sub = test[[\"ID\"]]\n",
    "sub[\"target\"] = test[\"compiled_leak\"]\n",
    "\n",
    "if not os.path.exists('submissions'):\n",
    "    os.mkdir('submissions')\n",
    "    \n",
    "sub.to_csv('submissions/baseline_submission_with_leaks_'+'_'.join(time.ctime().split())+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
