{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from scipy.stats import mode, skew, kurtosis, entropy\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm.pandas(tqdm_notebook)\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohan/miniconda3/envs/research/lib/python3.6/site-packages/pandas/io/feather_format.py:112: FutureWarning: `nthreads` argument is deprecated, pass `use_threads` instead\n",
      "  return feather.read_dataframe(path, nthreads=nthreads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data loaded from feather 2.92 s!\n",
      "test data loaded from feather 22.50 s!\n"
     ]
    }
   ],
   "source": [
    "def load_data(conf):\n",
    "    st = time.time()\n",
    "    if os.path.exists(f'{conf}.feather'):\n",
    "        out = pd.read_feather(f'{conf}.feather')\n",
    "        print(f'{conf} data loaded from feather {time.time()-st:.2f} s!')\n",
    "    else:\n",
    "        out = pd.read_csv(f'./{conf}.csv.zip')\n",
    "        print(f'{conf} data loaded from csv {time.time()-st:.2f} s!')\n",
    "        out.to_feather(f'{conf}.feather')\n",
    "        print(f'{conf} data save to feather {time.time()-st:.2f} s!')  \n",
    "    return out \n",
    "train = load_data('train')\n",
    "test = load_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transact_cols = [f for f in train.columns if f not in [\"ID\", \"target\"]]\n",
    "y = np.log1p(train[\"target\"]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['f190486d6', '58e2e02e6', 'eeb9cd3aa', '9fd594eec', '6eef030c1',\n",
    "       '15ace8c9f', 'fb0f5dbfe', '58e056e12', '20aa07010', '024c577b9',\n",
    "       'd6bb78916', 'b43a7cfd5', '58232a6fb', '1702b5bf0', '324921c7b', \n",
    "       '62e59a501', '2ec5b290f', '241f0f867', 'fb49e4212',  '66ace2992',\n",
    "       'f74e8f13d', '5c6487af1', '963a49cdc', '26fc93eb7', '1931ccfdd', \n",
    "       '703885424', '70feb1494', '491b9ee45', '23310aa6f', 'e176a204a',\n",
    "       '6619d81fc', '1db387535', 'fc99f9426', '91f701ba2',  '0572565c2',\n",
    "       '190db8488',  'adb64ff71', 'c47340d97', 'c5a231d81', '0ff32eb98'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.2 ms, sys: 46.9 ms, total: 100 ms\n",
      "Wall time: 86.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test[\"target\"] = train[\"target\"].mean()\n",
    "\n",
    "all_df = pd.concat([train[[\"ID\", \"target\"] + cols], test[[\"ID\", \"target\"]+ cols]]).reset_index(drop=True)\n",
    "all_df.head()\n",
    "all_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "CPU_CORES = 1\n",
    "def _time_series_info(row):\n",
    "    try:\n",
    "        id_1st_nz = row.nonzero()[0][0]\n",
    "        value_1st_nz = str(row[id_1st_nz])\n",
    "    except:\n",
    "        return '0_0.0'   \n",
    "    return str(id_1st_nz)+'_'+value_1st_nz\n",
    "\n",
    "def _join2str(df):\n",
    "    return df.apply(lambda x: \"_\".join(x.round(2).astype(str)), axis=1)\n",
    "\n",
    "def _get_leak(df, cols, lag=0, n_thread=2):\n",
    "    \"\"\"\n",
    "    Get leaked data\n",
    "    \"\"\"\n",
    "    st = time.time()\n",
    "    \n",
    "    df_split = np.array_split(df[cols[lag+2:]], n_thread)\n",
    "    df_shift_split = np.array_split(df[cols].shift(lag+2, axis=1)[cols[lag+2:]], n_thread)\n",
    "    \n",
    "    print(f'Shift columns: {time.time()-st:.2f} seconds!')\n",
    "    with Pool(processes=n_thread) as p:\n",
    "        result1 = p.map(_join2str, df_split)\n",
    "        result2 = p.map(_join2str, df_shift_split)\n",
    "        \n",
    "    series_str = pd.concat(list(result1), ignore_index=True)\n",
    "    series_shifted_str = pd.concat(list(result2), ignore_index=True)\n",
    "#     series_str = df[cols[lag+2:]].apply(lambda x: \"_\".join(x.round(2).astype(str)), axis=1)\n",
    "#     series_shifted_str = df[cols].shift(lag+2, axis=1)[cols[lag+2:]].apply(lambda x: \n",
    "#                                                                            \"_\".join(x.round(2).astype(str)), \n",
    "#                                                                            axis=1)   \n",
    "    print(f'Create time series strings before and after shift: {time.time()-st:.2f} seconds!')\n",
    "    \n",
    "    st = time.time()\n",
    "    series_dict = {}\n",
    "    for i in range(len(series_str)):\n",
    "        key = series_str[i]\n",
    "        if key in series_dict.keys():\n",
    "            continue\n",
    "        series_dict[key] = i\n",
    "\n",
    "    print(f'Create dictionary for faster search: {time.time()-st:.2f} seconds!')\n",
    "    \n",
    "    st = time.time()\n",
    "    target_vals = series_shifted_str.apply(lambda x: df.loc[series_dict[x], cols[lag]] \n",
    "                                                   if x in series_dict else 0)\n",
    "    print(f'Matching process finished: {time.time()-st:.2f} seconds!')\n",
    "#     target_rows = series_shifted_str.progress_apply(lambda x: series_dict[x])\n",
    "#     target_vals = target_rows.apply(lambda x: df.loc[x, cols[lag]] if len(x)==1 else 0)\n",
    "    return target_vals\n",
    "\n",
    "#     target_rows = series_shifted_str.progress_apply(lambda x: np.where(x == series_str)[0])\n",
    "#     target_vals = target_rows.apply(lambda x: df.loc[x[0], cols[lag]] if len(x)==1 else 0)\n",
    "#     return target_vals\n",
    "\n",
    "def get_all_leak(df, cols=None, nlags=15):\n",
    "    \"\"\"\n",
    "    We just recursively fetch target value for different lags\n",
    "    \"\"\"\n",
    "    df =  df.copy()\n",
    "    \n",
    "    for i in range(nlags):\n",
    "        print(\"Processing lag {}\".format(i))\n",
    "        df[\"leaked_target_\"+str(i)] = _get_leak(df, cols, i)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shift columns: 0.15 seconds!\n",
      "Create time series strings before and after shift: 13.63 seconds!\n",
      "Create dictionary for faster search: 0.78 seconds!\n",
      "Matching process finished: 0.41 seconds!\n",
      "CPU times: user 1.5 s, sys: 302 ms, total: 1.8 s\n",
      "Wall time: 14.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## test the speed of get leaked data of one lag value\n",
    "d = _get_leak(all_df, cols, 0)\n",
    "test_ = all_df\n",
    "test_['predict'] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lag 0\n",
      "Create time series strings before and after shift: 21.74 seconds!\n",
      "Create dictionary for faster search: 0.68 seconds!\n",
      "Matching process finished: 0.36 seconds!\n",
      "Processing lag 1\n",
      "Create time series strings before and after shift: 21.55 seconds!\n",
      "Create dictionary for faster search: 0.67 seconds!\n",
      "Matching process finished: 0.36 seconds!\n",
      "Processing lag 2\n",
      "Create time series strings before and after shift: 24.84 seconds!\n",
      "Create dictionary for faster search: 0.75 seconds!\n",
      "Matching process finished: 0.39 seconds!\n",
      "Processing lag 3\n",
      "Create time series strings before and after shift: 20.86 seconds!\n",
      "Create dictionary for faster search: 0.73 seconds!\n",
      "Matching process finished: 0.43 seconds!\n",
      "Processing lag 4\n",
      "Create time series strings before and after shift: 22.76 seconds!\n",
      "Create dictionary for faster search: 0.70 seconds!\n",
      "Matching process finished: 0.38 seconds!\n",
      "Processing lag 5\n",
      "Create time series strings before and after shift: 21.91 seconds!\n",
      "Create dictionary for faster search: 0.83 seconds!\n",
      "Matching process finished: 0.39 seconds!\n",
      "Processing lag 6\n",
      "Create time series strings before and after shift: 24.41 seconds!\n",
      "Create dictionary for faster search: 0.78 seconds!\n",
      "Matching process finished: 0.43 seconds!\n",
      "Processing lag 7\n",
      "Create time series strings before and after shift: 24.93 seconds!\n",
      "Create dictionary for faster search: 0.81 seconds!\n",
      "Matching process finished: 0.40 seconds!\n",
      "Processing lag 8\n",
      "Create time series strings before and after shift: 22.90 seconds!\n",
      "Create dictionary for faster search: 0.72 seconds!\n",
      "Matching process finished: 0.36 seconds!\n",
      "Processing lag 9\n",
      "Create time series strings before and after shift: 24.05 seconds!\n",
      "Create dictionary for faster search: 1.06 seconds!\n",
      "Matching process finished: 0.44 seconds!\n",
      "Processing lag 10\n",
      "Create time series strings before and after shift: 24.31 seconds!\n",
      "Create dictionary for faster search: 0.72 seconds!\n",
      "Matching process finished: 0.37 seconds!\n",
      "Processing lag 11\n",
      "Create time series strings before and after shift: 23.91 seconds!\n",
      "Create dictionary for faster search: 0.91 seconds!\n",
      "Matching process finished: 0.38 seconds!\n",
      "Processing lag 12\n",
      "Create time series strings before and after shift: 24.80 seconds!\n",
      "Create dictionary for faster search: 1.09 seconds!\n",
      "Matching process finished: 0.53 seconds!\n",
      "Processing lag 13\n",
      "Create time series strings before and after shift: 24.46 seconds!\n",
      "Create dictionary for faster search: 0.68 seconds!\n",
      "Matching process finished: 0.37 seconds!\n",
      "Processing lag 14\n",
      "Create time series strings before and after shift: 20.21 seconds!\n",
      "Create dictionary for faster search: 0.66 seconds!\n",
      "Matching process finished: 0.37 seconds!\n",
      "Processing lag 15\n",
      "Create time series strings before and after shift: 23.45 seconds!\n",
      "Create dictionary for faster search: 0.86 seconds!\n",
      "Matching process finished: 0.48 seconds!\n",
      "Processing lag 16\n",
      "Create time series strings before and after shift: 21.66 seconds!\n",
      "Create dictionary for faster search: 0.81 seconds!\n",
      "Matching process finished: 0.45 seconds!\n",
      "Processing lag 17\n",
      "Create time series strings before and after shift: 23.36 seconds!\n",
      "Create dictionary for faster search: 0.66 seconds!\n",
      "Matching process finished: 0.41 seconds!\n",
      "Processing lag 18\n",
      "Create time series strings before and after shift: 21.50 seconds!\n",
      "Create dictionary for faster search: 0.70 seconds!\n",
      "Matching process finished: 0.41 seconds!\n",
      "Processing lag 19\n",
      "Create time series strings before and after shift: 22.59 seconds!\n",
      "Create dictionary for faster search: 0.72 seconds!\n",
      "Matching process finished: 0.40 seconds!\n",
      "Processing lag 20\n",
      "Create time series strings before and after shift: 20.38 seconds!\n",
      "Create dictionary for faster search: 0.65 seconds!\n",
      "Matching process finished: 0.38 seconds!\n",
      "Processing lag 21\n",
      "Create time series strings before and after shift: 21.14 seconds!\n",
      "Create dictionary for faster search: 0.71 seconds!\n",
      "Matching process finished: 0.38 seconds!\n",
      "Processing lag 22\n",
      "Create time series strings before and after shift: 19.87 seconds!\n",
      "Create dictionary for faster search: 0.65 seconds!\n",
      "Matching process finished: 0.41 seconds!\n",
      "Processing lag 23\n",
      "Create time series strings before and after shift: 19.96 seconds!\n",
      "Create dictionary for faster search: 0.67 seconds!\n",
      "Matching process finished: 0.39 seconds!\n",
      "Processing lag 24\n",
      "Create time series strings before and after shift: 22.54 seconds!\n",
      "Create dictionary for faster search: 0.69 seconds!\n",
      "Matching process finished: 0.39 seconds!\n",
      "CPU times: user 9min 41s, sys: 5.99 s, total: 9min 47s\n",
      "Wall time: 9min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NLAGS = 25 #Increasing this might help push score a bit\n",
    "all_df = get_all_leak(all_df, cols=cols, nlags=NLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns overlap but no suffix specified: Index(['leaked_target_0', 'leaked_target_1', 'leaked_target_2',\n       'leaked_target_3', 'leaked_target_4', 'leaked_target_5',\n       'leaked_target_6', 'leaked_target_7', 'leaked_target_8',\n       'leaked_target_9', 'leaked_target_10', 'leaked_target_11',\n       'leaked_target_12', 'leaked_target_13', 'leaked_target_14',\n       'leaked_target_15', 'leaked_target_16', 'leaked_target_17',\n       'leaked_target_18', 'leaked_target_19', 'leaked_target_20',\n       'leaked_target_21', 'leaked_target_22', 'leaked_target_23',\n       'leaked_target_24'],\n      dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   6334\u001b[0m         \u001b[0;31m# For SparseDataFrame's benefit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6335\u001b[0m         return self._join_compat(other, on=on, how=how, lsuffix=lsuffix,\n\u001b[0;32m-> 6336\u001b[0;31m                                  rsuffix=rsuffix, sort=sort)\n\u001b[0m\u001b[1;32m   6337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6338\u001b[0m     def _join_compat(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_join_compat\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   6349\u001b[0m             return merge(self, other, left_on=on, how=how,\n\u001b[1;32m   6350\u001b[0m                          \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6351\u001b[0;31m                          suffixes=(lsuffix, rsuffix), sort=sort)\n\u001b[0m\u001b[1;32m   6352\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     60\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                          validate=validate)\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         llabels, rlabels = items_overlap_with_suffix(ldata.items, lsuf,\n\u001b[0;32m--> 574\u001b[0;31m                                                      rdata.items, rsuf)\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0mlindexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mleft_indexer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mitems_overlap_with_suffix\u001b[0;34m(left, lsuffix, right, rsuffix)\u001b[0m\n\u001b[1;32m   5242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlsuffix\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5243\u001b[0m             raise ValueError('columns overlap but no suffix specified: '\n\u001b[0;32m-> 5244\u001b[0;31m                              '{rename}'.format(rename=to_rename))\n\u001b[0m\u001b[1;32m   5245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5246\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mlrenamer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: columns overlap but no suffix specified: Index(['leaked_target_0', 'leaked_target_1', 'leaked_target_2',\n       'leaked_target_3', 'leaked_target_4', 'leaked_target_5',\n       'leaked_target_6', 'leaked_target_7', 'leaked_target_8',\n       'leaked_target_9', 'leaked_target_10', 'leaked_target_11',\n       'leaked_target_12', 'leaked_target_13', 'leaked_target_14',\n       'leaked_target_15', 'leaked_target_16', 'leaked_target_17',\n       'leaked_target_18', 'leaked_target_19', 'leaked_target_20',\n       'leaked_target_21', 'leaked_target_22', 'leaked_target_23',\n       'leaked_target_24'],\n      dtype='object')"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "leaky_cols = [\"leaked_target_\"+str(i) for i in range(NLAGS)]\n",
    "train = train.join(all_df.set_index(\"ID\")[leaky_cols], on=\"ID\", how=\"left\")\n",
    "test = test.join(all_df.set_index(\"ID\")[leaky_cols], on=\"ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## post-processing\n",
    "cnt = 0\n",
    "for i in range(4300):\n",
    "    tar = train['target'].iloc[i]\n",
    "#     lst = train[leaky_cols].iloc[i].unique()\n",
    "#     if tar in lst:\n",
    "#         print(f'True, {len(lst)}, {lst}')\n",
    "#     else:\n",
    "#         print(f'False, {len(lst)}, {lst}')\n",
    "    nz_idx = train[leaky_cols].iloc[i].nonzero()[0]\n",
    "    try:\n",
    "        likely = train[leaky_cols].iloc[i][nz_idx].value_counts().reset_index().sort_values(by=i, ascending=False).iloc[0,0]\n",
    "    except:\n",
    "        likely = 0.0\n",
    "#     print(likely)\n",
    "    if tar == likely:\n",
    "#         print(f'True')\n",
    "        cnt += 1\n",
    "#     else:\n",
    "#         print(f'False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3366\n"
     ]
    }
   ],
   "source": [
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3589.0"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3700*0.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train[\"nonzero_mean\"] = train[transact_cols].apply(lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1)\n",
    "test[\"nonzero_mean\"] = test[transact_cols].apply(lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leak values found in train and test  4072 32698\n",
      "% of correct leaks values in train  0.8747544204322201\n",
      "CPU times: user 5.13 s, sys: 7.08 s, total: 12.2 s\n",
      "Wall time: 9.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#We start with 1st lag target and recusrsively fill zero's\n",
    "train[\"compiled_leak\"] = 0\n",
    "test[\"compiled_leak\"] = 0\n",
    "for i in range(NLAGS):\n",
    "    train.loc[train[\"compiled_leak\"] == 0, \"compiled_leak\"] = train.loc[train[\"compiled_leak\"] == 0, \"leaked_target_\"+str(i)]\n",
    "    test.loc[test[\"compiled_leak\"] == 0, \"compiled_leak\"] = test.loc[test[\"compiled_leak\"] == 0, \"leaked_target_\"+str(i)]\n",
    "    \n",
    "print(\"Leak values found in train and test \", sum(train[\"compiled_leak\"] > 0), sum(test[\"compiled_leak\"] > 0))\n",
    "print(\"% of correct leaks values in train \", sum(train[\"compiled_leak\"] == train[\"target\"])/sum(train[\"compiled_leak\"] > 0))\n",
    "\n",
    "train.loc[train[\"compiled_leak\"] == 0, \"compiled_leak\"] = train.loc[train[\"compiled_leak\"] == 0, \"nonzero_mean\"]\n",
    "test.loc[test[\"compiled_leak\"] == 0, \"compiled_leak\"] = test.loc[test[\"compiled_leak\"] == 0, \"nonzero_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8014511083501742"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "np.sqrt(mean_squared_error(y, np.log1p(train[\"compiled_leak\"]).fillna(14.49)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liumohan/miniconda3/envs/datascience/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "sub = test[[\"ID\"]]\n",
    "sub[\"target\"] = test[\"compiled_leak\"]\n",
    "sub.to_csv(\"baseline_submission_with_leaks.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('./baseline_submission_with_leaks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49342, 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
