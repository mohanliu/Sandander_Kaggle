{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohan/miniconda3/envs/datascience/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/mohan/miniconda3/envs/datascience/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/mohan/miniconda3/envs/datascience/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "# import lightgbm as lgb\n",
    "# from sklearn.model_selection import *\n",
    "# from sklearn.metrics import mean_squared_error, make_scorer\n",
    "# from scipy.stats import mode, skew, kurtosis, entropy\n",
    "# from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# import dask.dataframe as dd\n",
    "# from dask.multiprocessing import get\n",
    "\n",
    "# from tqdm import tqdm, tqdm_notebook\n",
    "# tqdm.pandas(tqdm_notebook)\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohan/miniconda3/envs/datascience/lib/python3.6/site-packages/pandas/io/feather_format.py:112: FutureWarning: `nthreads` argument is deprecated, pass `use_threads` instead\n",
      "  return feather.read_dataframe(path, nthreads=nthreads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data loaded from feather 3.65 s!\n",
      "test data loaded from feather 16.84 s!\n"
     ]
    }
   ],
   "source": [
    "def load_data(conf):\n",
    "    st = time.time()\n",
    "    if os.path.exists(f'{conf}.feather'):\n",
    "        out = pd.read_feather(f'{conf}.feather')\n",
    "        print(f'{conf} data loaded from feather {time.time()-st:.2f} s!')\n",
    "    else:\n",
    "        out = pd.read_csv(f'./{conf}.csv.zip')\n",
    "        print(f'{conf} data loaded from csv {time.time()-st:.2f} s!')\n",
    "        out.to_feather(f'{conf}.feather')\n",
    "        print(f'{conf} data save to feather {time.time()-st:.2f} s!')  \n",
    "    return out \n",
    "train = load_data('train')\n",
    "test = load_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transact_cols = [f for f in train.columns if f not in [\"ID\", \"target\"]]\n",
    "y = np.log1p(train[\"target\"]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['f190486d6', '58e2e02e6', 'eeb9cd3aa', '9fd594eec', '6eef030c1',\n",
    "       '15ace8c9f', 'fb0f5dbfe', '58e056e12', '20aa07010', '024c577b9',\n",
    "       'd6bb78916', 'b43a7cfd5', '58232a6fb', '1702b5bf0', '324921c7b', \n",
    "       '62e59a501', '2ec5b290f', '241f0f867', 'fb49e4212',  '66ace2992',\n",
    "       'f74e8f13d', '5c6487af1', '963a49cdc', '26fc93eb7', '1931ccfdd', \n",
    "       '703885424', '70feb1494', '491b9ee45', '23310aa6f', 'e176a204a',\n",
    "       '6619d81fc', '1db387535', 'fc99f9426', '91f701ba2',  '0572565c2',\n",
    "       '190db8488',  'adb64ff71', 'c47340d97', 'c5a231d81', '0ff32eb98'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.22 s, sys: 1.24 s, total: 2.45 s\n",
      "Wall time: 1.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test[\"target\"] = train[\"target\"].mean()\n",
    "\n",
    "all_df = pd.concat([train[[\"ID\", \"target\"] + cols], test[[\"ID\", \"target\"]+ cols]]).reset_index(drop=True)\n",
    "all_df.head()\n",
    "all_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "def _time_series_info(row):\n",
    "    try:\n",
    "        id_1st_nz = row.nonzero()[0][0]\n",
    "        value_1st_nz = str(row[id_1st_nz])\n",
    "    except:\n",
    "        return '0_0.0'   \n",
    "    return str(id_1st_nz)+'_'+value_1st_nz\n",
    "\n",
    "def _join2str(df):\n",
    "    return df.apply(lambda x: \"_\".join(x.round(2).astype(str)), axis=1)\n",
    "\n",
    "def _get_leak(df, cols, lag=0, n_thread=4):\n",
    "    \"\"\"\n",
    "    Get leaked data\n",
    "    \"\"\"\n",
    "    st = time.time()\n",
    "    \n",
    "    df_split = np.array_split(df[cols[lag+2:]], n_thread)\n",
    "    df_shift_split = np.array_split(df[cols].shift(lag+2, axis=1)[cols[lag+2:]], n_thread)\n",
    "    \n",
    "    print(f'Shift columns: {time.time()-st:.2f} seconds!')\n",
    "    with Pool(processes=n_thread) as p:\n",
    "        result1 = p.map(_join2str, df_split)\n",
    "        result2 = p.map(_join2str, df_shift_split)\n",
    "        \n",
    "    series_str = pd.concat(list(result1), ignore_index=True)\n",
    "    series_shifted_str = pd.concat(list(result2), ignore_index=True)\n",
    "    print(f'Create time series strings before and after shift: {time.time()-st:.2f} seconds!')\n",
    "    \n",
    "    st = time.time()\n",
    "    series_dict = {}\n",
    "    for i in range(len(series_str)):\n",
    "        key = series_str[i]\n",
    "        if key in series_dict.keys():\n",
    "            continue\n",
    "        series_dict[key] = i\n",
    "    print(f'Create dictionary for faster search: {time.time()-st:.2f} seconds!')\n",
    "    \n",
    "    st = time.time()\n",
    "    target_vals = series_shifted_str.apply(lambda x: df.loc[series_dict[x], cols[lag]] \n",
    "                                                   if x in series_dict else 0)\n",
    "    print(f'Matching process finished: {time.time()-st:.2f} seconds!')\n",
    "    return target_vals\n",
    "\n",
    "def get_all_leak(df, cols=None, nlags=15):\n",
    "    \"\"\"\n",
    "    We just recursively fetch target value for different lags\n",
    "    \"\"\"\n",
    "    df =  df.copy()\n",
    "    \n",
    "    for i in range(nlags):\n",
    "        print(\"Processing lag {}\".format(i))\n",
    "        df[\"leaked_target_\"+str(i)] = _get_leak(df, cols, i)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lag 0\n",
      "Shift columns: 0.05 seconds!\n",
      "Create time series strings before and after shift: 5.32 seconds!\n",
      "Create dictionary for faster search: 0.70 seconds!\n",
      "Matching process finished: 0.35 seconds!\n",
      "Processing lag 1\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 5.09 seconds!\n",
      "Create dictionary for faster search: 0.67 seconds!\n",
      "Matching process finished: 0.34 seconds!\n",
      "Processing lag 2\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 4.29 seconds!\n",
      "Create dictionary for faster search: 0.69 seconds!\n",
      "Matching process finished: 0.34 seconds!\n",
      "Processing lag 3\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 4.69 seconds!\n",
      "Create dictionary for faster search: 0.71 seconds!\n",
      "Matching process finished: 0.34 seconds!\n",
      "Processing lag 4\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 4.18 seconds!\n",
      "Create dictionary for faster search: 0.67 seconds!\n",
      "Matching process finished: 0.34 seconds!\n",
      "Processing lag 5\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 5.08 seconds!\n",
      "Create dictionary for faster search: 0.65 seconds!\n",
      "Matching process finished: 0.34 seconds!\n",
      "Processing lag 6\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 5.69 seconds!\n",
      "Create dictionary for faster search: 0.67 seconds!\n",
      "Matching process finished: 0.34 seconds!\n",
      "Processing lag 7\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 4.19 seconds!\n",
      "Create dictionary for faster search: 0.69 seconds!\n",
      "Matching process finished: 0.34 seconds!\n",
      "Processing lag 8\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 4.38 seconds!\n",
      "Create dictionary for faster search: 0.68 seconds!\n",
      "Matching process finished: 0.34 seconds!\n",
      "Processing lag 9\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 5.18 seconds!\n",
      "Create dictionary for faster search: 0.65 seconds!\n",
      "Matching process finished: 0.34 seconds!\n",
      "Processing lag 10\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 4.68 seconds!\n",
      "Create dictionary for faster search: 0.64 seconds!\n",
      "Matching process finished: 0.34 seconds!\n",
      "Processing lag 11\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 4.19 seconds!\n",
      "Create dictionary for faster search: 0.73 seconds!\n",
      "Matching process finished: 0.34 seconds!\n",
      "Processing lag 12\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 4.08 seconds!\n",
      "Create dictionary for faster search: 0.66 seconds!\n",
      "Matching process finished: 0.34 seconds!\n",
      "Processing lag 13\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 4.68 seconds!\n",
      "Create dictionary for faster search: 0.65 seconds!\n",
      "Matching process finished: 0.35 seconds!\n",
      "Processing lag 14\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 4.08 seconds!\n",
      "Create dictionary for faster search: 0.68 seconds!\n",
      "Matching process finished: 0.35 seconds!\n",
      "Processing lag 15\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 4.29 seconds!\n",
      "Create dictionary for faster search: 0.69 seconds!\n",
      "Matching process finished: 0.35 seconds!\n",
      "Processing lag 16\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 4.09 seconds!\n",
      "Create dictionary for faster search: 0.68 seconds!\n",
      "Matching process finished: 0.35 seconds!\n",
      "Processing lag 17\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 5.02 seconds!\n",
      "Create dictionary for faster search: 0.84 seconds!\n",
      "Matching process finished: 0.36 seconds!\n",
      "Processing lag 18\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 4.39 seconds!\n",
      "Create dictionary for faster search: 0.65 seconds!\n",
      "Matching process finished: 0.35 seconds!\n",
      "Processing lag 19\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 4.11 seconds!\n",
      "Create dictionary for faster search: 0.73 seconds!\n",
      "Matching process finished: 0.36 seconds!\n",
      "Processing lag 20\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 5.67 seconds!\n",
      "Create dictionary for faster search: 0.65 seconds!\n",
      "Matching process finished: 0.36 seconds!\n",
      "Processing lag 21\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 4.38 seconds!\n",
      "Create dictionary for faster search: 0.75 seconds!\n",
      "Matching process finished: 0.42 seconds!\n",
      "Processing lag 22\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 4.47 seconds!\n",
      "Create dictionary for faster search: 0.64 seconds!\n",
      "Matching process finished: 0.37 seconds!\n",
      "Processing lag 23\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 4.70 seconds!\n",
      "Create dictionary for faster search: 0.65 seconds!\n",
      "Matching process finished: 0.37 seconds!\n",
      "Processing lag 24\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 5.39 seconds!\n",
      "Create dictionary for faster search: 0.65 seconds!\n",
      "Matching process finished: 0.37 seconds!\n",
      "Processing lag 25\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 4.29 seconds!\n",
      "Create dictionary for faster search: 0.68 seconds!\n",
      "Matching process finished: 0.38 seconds!\n",
      "Processing lag 26\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 4.99 seconds!\n",
      "Create dictionary for faster search: 0.66 seconds!\n",
      "Matching process finished: 0.38 seconds!\n",
      "Processing lag 27\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 3.98 seconds!\n",
      "Create dictionary for faster search: 0.65 seconds!\n",
      "Matching process finished: 0.38 seconds!\n",
      "Processing lag 28\n",
      "Shift columns: 0.03 seconds!\n",
      "Create time series strings before and after shift: 3.97 seconds!\n",
      "Create dictionary for faster search: 0.64 seconds!\n",
      "Matching process finished: 0.39 seconds!\n",
      "Processing lag 29\n",
      "Shift columns: 0.03 seconds!\n",
      "Create time series strings before and after shift: 4.07 seconds!\n",
      "Create dictionary for faster search: 0.65 seconds!\n",
      "Matching process finished: 0.40 seconds!\n",
      "Processing lag 30\n",
      "Shift columns: 0.03 seconds!\n",
      "Create time series strings before and after shift: 4.27 seconds!\n",
      "Create dictionary for faster search: 0.66 seconds!\n",
      "Matching process finished: 0.41 seconds!\n",
      "Processing lag 31\n",
      "Shift columns: 0.03 seconds!\n",
      "Create time series strings before and after shift: 5.19 seconds!\n",
      "Create dictionary for faster search: 0.67 seconds!\n",
      "Matching process finished: 0.42 seconds!\n",
      "Processing lag 32\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 4.08 seconds!\n",
      "Create dictionary for faster search: 0.64 seconds!\n",
      "Matching process finished: 0.43 seconds!\n",
      "Processing lag 33\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 4.29 seconds!\n",
      "Create dictionary for faster search: 0.70 seconds!\n",
      "Matching process finished: 0.46 seconds!\n",
      "Processing lag 34\n",
      "Shift columns: 0.03 seconds!\n",
      "Create time series strings before and after shift: 4.18 seconds!\n",
      "Create dictionary for faster search: 0.68 seconds!\n",
      "Matching process finished: 0.48 seconds!\n",
      "Processing lag 35\n",
      "Shift columns: 0.03 seconds!\n",
      "Create time series strings before and after shift: 5.30 seconds!\n",
      "Create dictionary for faster search: 0.74 seconds!\n",
      "Matching process finished: 0.51 seconds!\n",
      "Processing lag 36\n",
      "Shift columns: 0.03 seconds!\n",
      "Create time series strings before and after shift: 4.18 seconds!\n",
      "Create dictionary for faster search: 0.67 seconds!\n",
      "Matching process finished: 0.53 seconds!\n",
      "Processing lag 37\n",
      "Shift columns: 0.03 seconds!\n",
      "Create time series strings before and after shift: 4.38 seconds!\n",
      "Create dictionary for faster search: 0.68 seconds!\n",
      "Matching process finished: 0.58 seconds!\n",
      "CPU times: user 1min 7s, sys: 6.59 s, total: 1min 14s\n",
      "Wall time: 3min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NLAGS = 38 #Increasing this might help push score a bit\n",
    "leaky_cols = [\"leaked_target_\"+str(i) for i in range(NLAGS)]\n",
    "all_df = get_all_leak(all_df, cols=cols, nlags=NLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 51s, sys: 784 ms, total: 2min 52s\n",
      "Wall time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## post-processing\n",
    "def _get_likely(row):\n",
    "    tar = row.target\n",
    "    nz_idx = row[leaky_cols].nonzero()[0]\n",
    "    try:\n",
    "        nz_cnt_df = row[leaky_cols][nz_idx].value_counts().reset_index()\n",
    "        col_name = nz_cnt_df.columns[1]\n",
    "        sorted_df = nz_cnt_df.sort_values(by=col_name, ascending=False)\n",
    "        if sorted_df.iloc[0, 1] == 1:\n",
    "            likely = 0.0\n",
    "            prob = 0.0\n",
    "            cnt = 0\n",
    "        else:\n",
    "            likely = sorted_df.iloc[0, 0]\n",
    "            cnt = sorted_df.iloc[0, 1]\n",
    "            prob = sorted_df.iloc[0, 1]/sorted_df[col_name].sum()\n",
    "            \n",
    "    except:\n",
    "        likely = 0.0\n",
    "        prob = 0.0\n",
    "        cnt = 0\n",
    "    return likely, cnt, prob\n",
    "\n",
    "likely_args = all_df.apply(lambda x: _get_likely(x), axis=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the most likely values to feather\n",
    "likely_df = pd.DataFrame()\n",
    "likely_df['ID'] = all_df['ID']\n",
    "likely_df['likely_value'] = likely_args.apply(lambda x:x[0])\n",
    "likely_df['likely_cnt'] = likely_args.apply(lambda x:x[1])\n",
    "likely_df['likely_prob'] = likely_args.apply(lambda x:x[2])\n",
    "likely_df.to_feather('most_likely_value.feather')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['likely_value'] = likely_args.apply(lambda x:x[0])\n",
    "\n",
    "train_processed = train.join(all_df.set_index(\"ID\")[leaky_cols+['likely_value']], on=\"ID\", how=\"left\")\n",
    "test_processed = test.join(all_df.set_index(\"ID\")[leaky_cols+['likely_value']], on=\"ID\", how=\"left\")\n",
    "\n",
    "train_processed[\"nonzero_mean\"] = train[transact_cols].apply(lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1)\n",
    "test_processed[\"nonzero_mean\"] = test[transact_cols].apply(lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#We start with 1st lag target and recusrsively fill zero's\n",
    "def _get_prediction(row):\n",
    "    if row.likely_value != 0:\n",
    "        return row.likely_value\n",
    "    else:\n",
    "        return row.nonzero_mean\n",
    "    \n",
    "train_processed['predict'] = train_processed.apply(lambda x: _get_prediction(x), axis=1)\n",
    "# test_processed['predict'] = test_processed.apply(lambda x: _get_prediction(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = test[[\"ID\"]]\n",
    "sub[\"target\"] = test[\"predict\"]\n",
    "\n",
    "if not os.path.exists('submissions'):\n",
    "    os.mkdir('submissions')\n",
    "    \n",
    "sub.to_csv('submissions/baseline_submission_with_leaks_'+'_'.join(time.ctime().split())+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
