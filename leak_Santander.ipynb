{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "import lightgbm as lgb\n",
    "# from sklearn.model_selection import *\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from scipy.stats import mode, skew, kurtosis, entropy\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm.pandas(tqdm_notebook)\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data loaded from feather 0.33 s!\n",
      "test data loaded from feather 1.24 s!\n"
     ]
    }
   ],
   "source": [
    "def load_data(conf):\n",
    "    st = time.time()\n",
    "    if os.path.exists(f'{conf}_raw.feather'):\n",
    "        out = pd.read_feather(f'{conf}_raw.feather')\n",
    "        print(f'{conf} data loaded from feather {time.time()-st:.2f} s!')\n",
    "    else:\n",
    "        out = pd.read_csv(f'./{conf}.csv.zip')\n",
    "        print(f'{conf} data loaded from csv {time.time()-st:.2f} s!')\n",
    "        out.to_feather(f'{conf}_raw.feather')\n",
    "        print(f'{conf} data save to feather {time.time()-st:.2f} s!')  \n",
    "    return out \n",
    "train = load_data('train')\n",
    "test = load_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transact_cols = [f for f in train.columns if f not in [\"ID\", \"target\"]]\n",
    "y = np.log1p(train[\"target\"]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['f190486d6', '58e2e02e6', 'eeb9cd3aa', '9fd594eec', '6eef030c1',\n",
    "       '15ace8c9f', 'fb0f5dbfe', '58e056e12', '20aa07010', '024c577b9',\n",
    "       'd6bb78916', 'b43a7cfd5', '58232a6fb', '1702b5bf0', '324921c7b', \n",
    "       '62e59a501', '2ec5b290f', '241f0f867', 'fb49e4212',  '66ace2992',\n",
    "       'f74e8f13d', '5c6487af1', '963a49cdc', '26fc93eb7', '1931ccfdd', \n",
    "       '703885424', '70feb1494', '491b9ee45', '23310aa6f', 'e176a204a',\n",
    "       '6619d81fc', '1db387535', 'fc99f9426', '91f701ba2',  '0572565c2',\n",
    "       '190db8488',  'adb64ff71', 'c47340d97', 'c5a231d81', '0ff32eb98'] \n",
    "# leak_col = []\n",
    "dataCols = [c for c in train.columns if c not in ['ID', 'target']]\n",
    "# for c in dataCols:\n",
    "#     leak2 = np.count_nonzero(( np.abs((train[c] - train['target']) / train['target']) < 0.05).astype(int))\n",
    "#     if leak2 > 50:\n",
    "#         leak_col.append(c)\n",
    "# print(len(leak_col))\n",
    "# cols = leak_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.22 s, sys: 1.86 s, total: 3.07 s\n",
      "Wall time: 1.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test[\"target\"] = train[\"target\"].mean()\n",
    "all_df = pd.concat([train[[\"ID\", \"target\"] + cols], test[[\"ID\", \"target\"]+ cols]]).reset_index(drop=True)\n",
    "all_df.head()\n",
    "all_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "CPU_CORES = cpu_count()\n",
    "def _time_series_info(row):\n",
    "    try:\n",
    "        id_1st_nz = row.nonzero()[0][0]\n",
    "        value_1st_nz = str(round(np.log1p(row[id_1st_nz]), 5))\n",
    "    except:\n",
    "        return 'na'\n",
    "    return str(id_1st_nz)+'_'+value_1st_nz\n",
    "\n",
    "def _join2str(df):\n",
    "    return df.apply(lambda x: \"_\".join(round(np.log1p(x), 5).astype(str)), axis=1)\n",
    "\n",
    "def _get_leak(df, cols, lag=0, n_thread=CPU_CORES):\n",
    "    \"\"\"\n",
    "    Get leaked data\n",
    "    \"\"\"\n",
    "    st = time.time()\n",
    "    \n",
    "    df_split = np.array_split(df[cols[lag+2:]], n_thread)\n",
    "    df_shift_split = np.array_split(df[cols].shift(lag+2, axis=1)[cols[lag+2:]], n_thread)\n",
    "    \n",
    "    print(f'Shift columns: {time.time()-st:.2f} seconds!')\n",
    "    with Pool(processes=n_thread) as p:\n",
    "        result1 = p.map(_join2str, df_split)\n",
    "        result2 = p.map(_join2str, df_shift_split)\n",
    "        \n",
    "    series_str = pd.concat(list(result1), ignore_index=True)\n",
    "    series_shifted_str = pd.concat(list(result2), ignore_index=True)\n",
    "    print(f'Create time series strings before and after shift: {time.time()-st:.2f} seconds!')\n",
    "    \n",
    "    st = time.time()\n",
    "    series_dict = {}\n",
    "    for i in range(len(series_str)):\n",
    "        key = series_str[i]\n",
    "        if key not in series_dict:\n",
    "            series_dict[key] = i\n",
    "\n",
    "    print(f'Create dictionary for faster search: {time.time()-st:.2f} seconds!')\n",
    "    \n",
    "    st = time.time()\n",
    "    target_vals = series_shifted_str.apply(lambda x: df.loc[series_dict[x], cols[lag]] \n",
    "                                                   if x in series_dict else 0)\n",
    "    print(f'Matching process finished: {time.time()-st:.2f} seconds!')\n",
    "    return target_vals\n",
    "\n",
    "#     target_rows = series_shifted_str.progress_apply(lambda x: np.where(x == series_str)[0])\n",
    "#     target_vals = target_rows.apply(lambda x: df.loc[x[0], cols[lag]] if len(x)==1 else 0)\n",
    "#     return target_vals\n",
    "\n",
    "def get_all_leak(df, cols=None, nlags=15):\n",
    "    \"\"\"\n",
    "    We just recursively fetch target value for different lags\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    for i in range(nlags):\n",
    "        print(\"Processing lag {}\".format(i))\n",
    "        df[\"leaked_target_\"+str(i)] = _get_leak(df, cols, i)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shift columns: 0.09 seconds!\n",
      "Create time series strings before and after shift: 4.75 seconds!\n",
      "Create dictionary for faster search: 0.41 seconds!\n",
      "Matching process finished: 0.18 seconds!\n",
      "CPU times: user 1.34 s, sys: 997 ms, total: 2.33 s\n",
      "Wall time: 5.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## test the speed of get leaked data of one lag value\n",
    "d = _get_leak(all_df, cols, 0)\n",
    "test_ = all_df\n",
    "test_['predict'] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lag 0\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 3.99 seconds!\n",
      "Create dictionary for faster search: 0.41 seconds!\n",
      "Matching process finished: 0.22 seconds!\n",
      "Processing lag 1\n",
      "Shift columns: 0.06 seconds!\n",
      "Create time series strings before and after shift: 4.01 seconds!\n",
      "Create dictionary for faster search: 0.41 seconds!\n",
      "Matching process finished: 0.21 seconds!\n",
      "Processing lag 2\n",
      "Shift columns: 0.05 seconds!\n",
      "Create time series strings before and after shift: 4.00 seconds!\n",
      "Create dictionary for faster search: 0.40 seconds!\n",
      "Matching process finished: 0.21 seconds!\n",
      "Processing lag 3\n",
      "Shift columns: 0.06 seconds!\n",
      "Create time series strings before and after shift: 3.92 seconds!\n",
      "Create dictionary for faster search: 0.40 seconds!\n",
      "Matching process finished: 0.22 seconds!\n",
      "Processing lag 4\n",
      "Shift columns: 0.05 seconds!\n",
      "Create time series strings before and after shift: 3.91 seconds!\n",
      "Create dictionary for faster search: 0.40 seconds!\n",
      "Matching process finished: 0.22 seconds!\n",
      "Processing lag 5\n",
      "Shift columns: 0.05 seconds!\n",
      "Create time series strings before and after shift: 3.91 seconds!\n",
      "Create dictionary for faster search: 0.40 seconds!\n",
      "Matching process finished: 0.21 seconds!\n",
      "Processing lag 6\n",
      "Shift columns: 0.05 seconds!\n",
      "Create time series strings before and after shift: 3.91 seconds!\n",
      "Create dictionary for faster search: 0.40 seconds!\n",
      "Matching process finished: 0.21 seconds!\n",
      "Processing lag 7\n",
      "Shift columns: 0.05 seconds!\n",
      "Create time series strings before and after shift: 3.91 seconds!\n",
      "Create dictionary for faster search: 0.40 seconds!\n",
      "Matching process finished: 0.22 seconds!\n",
      "Processing lag 8\n",
      "Shift columns: 0.05 seconds!\n",
      "Create time series strings before and after shift: 3.91 seconds!\n",
      "Create dictionary for faster search: 0.41 seconds!\n",
      "Matching process finished: 0.22 seconds!\n",
      "Processing lag 9\n",
      "Shift columns: 0.05 seconds!\n",
      "Create time series strings before and after shift: 3.90 seconds!\n",
      "Create dictionary for faster search: 0.40 seconds!\n",
      "Matching process finished: 0.22 seconds!\n",
      "Processing lag 10\n",
      "Shift columns: 0.05 seconds!\n",
      "Create time series strings before and after shift: 3.90 seconds!\n",
      "Create dictionary for faster search: 0.40 seconds!\n",
      "Matching process finished: 0.22 seconds!\n",
      "Processing lag 11\n",
      "Shift columns: 0.05 seconds!\n",
      "Create time series strings before and after shift: 3.91 seconds!\n",
      "Create dictionary for faster search: 0.40 seconds!\n",
      "Matching process finished: 0.22 seconds!\n",
      "Processing lag 12\n",
      "Shift columns: 0.05 seconds!\n",
      "Create time series strings before and after shift: 3.93 seconds!\n",
      "Create dictionary for faster search: 0.40 seconds!\n",
      "Matching process finished: 0.22 seconds!\n",
      "Processing lag 13\n",
      "Shift columns: 0.05 seconds!\n",
      "Create time series strings before and after shift: 3.92 seconds!\n",
      "Create dictionary for faster search: 0.40 seconds!\n",
      "Matching process finished: 0.22 seconds!\n",
      "Processing lag 14\n",
      "Shift columns: 0.05 seconds!\n",
      "Create time series strings before and after shift: 3.81 seconds!\n",
      "Create dictionary for faster search: 0.40 seconds!\n",
      "Matching process finished: 0.22 seconds!\n",
      "Processing lag 15\n",
      "Shift columns: 0.05 seconds!\n",
      "Create time series strings before and after shift: 3.80 seconds!\n",
      "Create dictionary for faster search: 0.40 seconds!\n",
      "Matching process finished: 0.22 seconds!\n",
      "Processing lag 16\n",
      "Shift columns: 0.05 seconds!\n",
      "Create time series strings before and after shift: 3.80 seconds!\n",
      "Create dictionary for faster search: 0.40 seconds!\n",
      "Matching process finished: 0.22 seconds!\n",
      "Processing lag 17\n",
      "Shift columns: 0.05 seconds!\n",
      "Create time series strings before and after shift: 3.80 seconds!\n",
      "Create dictionary for faster search: 0.40 seconds!\n",
      "Matching process finished: 0.22 seconds!\n",
      "Processing lag 18\n",
      "Shift columns: 0.05 seconds!\n",
      "Create time series strings before and after shift: 3.80 seconds!\n",
      "Create dictionary for faster search: 0.40 seconds!\n",
      "Matching process finished: 0.22 seconds!\n",
      "Processing lag 19\n",
      "Shift columns: 0.05 seconds!\n",
      "Create time series strings before and after shift: 3.82 seconds!\n",
      "Create dictionary for faster search: 0.39 seconds!\n",
      "Matching process finished: 0.22 seconds!\n",
      "Processing lag 20\n",
      "Shift columns: 0.05 seconds!\n",
      "Create time series strings before and after shift: 3.80 seconds!\n",
      "Create dictionary for faster search: 0.40 seconds!\n",
      "Matching process finished: 0.23 seconds!\n",
      "Processing lag 21\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 3.79 seconds!\n",
      "Create dictionary for faster search: 0.40 seconds!\n",
      "Matching process finished: 0.23 seconds!\n",
      "Processing lag 22\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 3.79 seconds!\n",
      "Create dictionary for faster search: 0.40 seconds!\n",
      "Matching process finished: 0.23 seconds!\n",
      "Processing lag 23\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 3.79 seconds!\n",
      "Create dictionary for faster search: 0.39 seconds!\n",
      "Matching process finished: 0.23 seconds!\n",
      "Processing lag 24\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 3.81 seconds!\n",
      "Create dictionary for faster search: 0.39 seconds!\n",
      "Matching process finished: 0.23 seconds!\n",
      "Processing lag 25\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 3.79 seconds!\n",
      "Create dictionary for faster search: 0.40 seconds!\n",
      "Matching process finished: 0.23 seconds!\n",
      "Processing lag 26\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 3.69 seconds!\n",
      "Create dictionary for faster search: 0.39 seconds!\n",
      "Matching process finished: 0.24 seconds!\n",
      "Processing lag 27\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 3.79 seconds!\n",
      "Create dictionary for faster search: 0.39 seconds!\n",
      "Matching process finished: 0.24 seconds!\n",
      "Processing lag 28\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 3.69 seconds!\n",
      "Create dictionary for faster search: 0.39 seconds!\n",
      "Matching process finished: 0.24 seconds!\n",
      "Processing lag 29\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 3.79 seconds!\n",
      "Create dictionary for faster search: 0.39 seconds!\n",
      "Matching process finished: 0.25 seconds!\n",
      "Processing lag 30\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 3.71 seconds!\n",
      "Create dictionary for faster search: 0.39 seconds!\n",
      "Matching process finished: 0.25 seconds!\n",
      "Processing lag 31\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 3.78 seconds!\n",
      "Create dictionary for faster search: 0.41 seconds!\n",
      "Matching process finished: 0.27 seconds!\n",
      "Processing lag 32\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 3.70 seconds!\n",
      "Create dictionary for faster search: 0.39 seconds!\n",
      "Matching process finished: 0.27 seconds!\n",
      "Processing lag 33\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 3.70 seconds!\n",
      "Create dictionary for faster search: 0.39 seconds!\n",
      "Matching process finished: 0.28 seconds!\n",
      "Processing lag 34\n",
      "Shift columns: 0.04 seconds!\n",
      "Create time series strings before and after shift: 3.69 seconds!\n",
      "Create dictionary for faster search: 0.39 seconds!\n",
      "Matching process finished: 0.29 seconds!\n",
      "CPU times: user 49.6 s, sys: 9.08 s, total: 58.6 s\n",
      "Wall time: 2min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NLAGS = 35 #Increasing this might help push score a bit\n",
    "all_df = get_all_leak(all_df, cols=cols, nlags=NLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.68 s, sys: 1.16 s, total: 2.84 s\n",
      "Wall time: 951 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "leaky_cols = [\"leaked_target_\"+str(i) for i in range(NLAGS)]\n",
    "train = train.join(all_df.set_index(\"ID\")[leaky_cols], on=\"ID\", how=\"left\")\n",
    "test = test.join(all_df.set_index(\"ID\")[leaky_cols], on=\"ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_feather(\"./train_leaky.feather\")\n",
    "test.to_feather(\"./test_leaky.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1866666.66"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_ = 1367\n",
    "nz_idx = train[leaky_cols].iloc[idx_].nonzero()[0]\n",
    "np.median(train[leaky_cols].iloc[idx_][nz_idx].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "likely = np.median(train[leaky_cols].iloc[idx_][nz_idx].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 s, sys: 6 ms, total: 13.1 s\n",
      "Wall time: 13.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## post-processing\n",
    "cnt = 0\n",
    "cntFuzzy = 0\n",
    "for i in range(4300):\n",
    "    tar = train['target'].iloc[i]\n",
    "    nz_idx = train[leaky_cols].iloc[i].nonzero()[0]\n",
    "    try:\n",
    "        candidate = np.median(train[leaky_cols].iloc[i][nz_idx].unique())\n",
    "        #candidate = train[leaky_cols].iloc[i][nz_idx].value_counts().reset_index().sort_values(by=i, ascending=False).iloc[0,0]\n",
    "        deviationPcent = np.abs(tar - candidate) / tar\n",
    "        if tar == candidate:\n",
    "            cnt += 1\n",
    "        if deviationPcent < 0.05:\n",
    "            cntFuzzy += 1\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3589.0"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3700*0.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train[\"nonzero_mean\"] = train[transact_cols].apply(lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1)\n",
    "test[\"nonzero_mean\"] = test[transact_cols].apply(lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leak values found in train and test  4072 32698\n",
      "% of correct leaks values in train  0.8747544204322201\n",
      "CPU times: user 5.13 s, sys: 7.08 s, total: 12.2 s\n",
      "Wall time: 9.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#We start with 1st lag target and recusrsively fill zero's\n",
    "train[\"compiled_leak\"] = 0\n",
    "test[\"compiled_leak\"] = 0\n",
    "for i in range(NLAGS):\n",
    "    train.loc[train[\"compiled_leak\"] == 0, \"compiled_leak\"] = train.loc[train[\"compiled_leak\"] == 0, \"leaked_target_\"+str(i)]\n",
    "    test.loc[test[\"compiled_leak\"] == 0, \"compiled_leak\"] = test.loc[test[\"compiled_leak\"] == 0, \"leaked_target_\"+str(i)]\n",
    "    \n",
    "print(\"Leak values found in train and test \", sum(train[\"compiled_leak\"] > 0), sum(test[\"compiled_leak\"] > 0))\n",
    "print(\"% of correct leaks values in train \", sum(train[\"compiled_leak\"] == train[\"target\"])/sum(train[\"compiled_leak\"] > 0))\n",
    "\n",
    "train.loc[train[\"compiled_leak\"] == 0, \"compiled_leak\"] = train.loc[train[\"compiled_leak\"] == 0, \"nonzero_mean\"]\n",
    "test.loc[test[\"compiled_leak\"] == 0, \"compiled_leak\"] = test.loc[test[\"compiled_leak\"] == 0, \"nonzero_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8014511083501742"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "np.sqrt(mean_squared_error(y, np.log1p(train[\"compiled_leak\"]).fillna(14.49)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liumohan/miniconda3/envs/datascience/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "sub = test[[\"ID\"]]\n",
    "sub[\"target\"] = test[\"compiled_leak\"]\n",
    "sub.to_csv(\"baseline_submission_with_leaks.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('./baseline_submission_with_leaks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49342, 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
